#!/usr/bin/env python3

# USAGE:   python tube.barcode.merge.py <Aliquot.xls> <MiniOutput.csv> <Metabolomics.xls> <all_inclusive.xlsx>

# sys.argv[1] = name of isotope aliquot file from in .xls format (or in .csv if it had to be modified)
# sys.argv[2] = name of SampleScanMini output file in .csv format
# sys.argv[3] = name of Metabolomics file generated by clarity.  Metabolomics adds isotope enrichment results to this file
# sys.argv[4] = name of all_inclusive.xlsx file of parent/source samples downloaded from Prospero

import pandas as pd
import numpy as np
import sys
import os
import xlrd
from xlutils.copy import copy
from os.path import exists as file_exists
from pathlib import Path
from sqlalchemy import create_engine



# define list of destination well positions for a 96-well
well_list_96w = ['A1', 'B1', 'C1', 'D1', 'E1', 'F1', 'G1', 'H1', 'A2', 'B2', 'C2', 'D2', 'E2', 'F2', 'G2', 'H2', 'A3', 'B3', 'C3',
                 'D3', 'E3', 'F3', 'G3', 'H3', 'A4', 'B4', 'C4', 'D4', 'E4', 'F4', 'G4', 'H4', 'A5', 'B5', 'C5', 'D5', 'E5', 'F5', 'G5',
                 'H5', 'A6', 'B6', 'C6', 'D6', 'E6', 'F6', 'G6', 'H6', 'A7', 'B7', 'C7', 'D7', 'E7', 'F7', 'G7', 'H7', 'A8', 'B8', 'C8',
                 'D8', 'E8', 'F8', 'G8', 'H8', 'A9', 'B9', 'C9', 'D9', 'E9', 'F9', 'G9', 'H9', 'A10', 'B10', 'C10', 'D10', 'E10', 'F10', 'G10',
                 'H10', 'A11', 'B11', 'C11', 'D11', 'E11', 'F11', 'G11', 'H11', 'A12', 'B12', 'C12', 'D12', 'E12', 'F12', 'G12', 'H12']


#########################
#########################
def createDirectories():
    # BASE_DIR = Path(__file__).parent
    BASE_DIR = Path.cwd()

    ISO_DIR = BASE_DIR / "1_setup_isotope_qc_fa"

    ISO_DIR.mkdir(parents=True, exist_ok=True)

    ISO_IN_DIR = ISO_DIR / "input_files"

    ISO_IN_DIR.mkdir(parents=True, exist_ok=True)

    ULTRA_DIR = BASE_DIR / "2_load_ultracentrifuge"

    ULTRA_DIR.mkdir(parents=True, exist_ok=True)

    MERGE_DIR = BASE_DIR / "3_merge_density_vol_conc_files"

    MERGE_DIR.mkdir(parents=True, exist_ok=True)

    LIB_DIR = BASE_DIR / "4_make_library_analyze_fa"

    LIB_DIR.mkdir(parents=True, exist_ok=True)

    FIRST_DIR = LIB_DIR / "A_first_attempt_make_lib"

    FIRST_DIR.mkdir(parents=True, exist_ok=True)

    FA_FIRST_DIR = LIB_DIR / "B_first_attempt_fa_result"

    FA_FIRST_DIR.mkdir(parents=True, exist_ok=True)

    SECOND_DIR = LIB_DIR / "C_second_attempt_make_lib"

    SECOND_DIR.mkdir(parents=True, exist_ok=True)

    FA_SECOND_DIR = LIB_DIR / "D_second_attempt_fa_result"

    FA_SECOND_DIR.mkdir(parents=True, exist_ok=True)

    POOL_DIR = BASE_DIR / "5_pooling"

    POOL_DIR.mkdir(parents=True, exist_ok=True)

    CLARITY_DIR = POOL_DIR / "A_make_clarity_aliquot_upload_file"

    CLARITY_DIR.mkdir(parents=True, exist_ok=True)

    CLAR_LIB_DIR = POOL_DIR / "B_fill_clarity_lib_creation_file"

    CLAR_LIB_DIR.mkdir(parents=True, exist_ok=True)

    ASSIGN_DIR = POOL_DIR / "C_assign_libs_to_pools"

    ASSIGN_DIR.mkdir(parents=True, exist_ok=True)

    FINISH_DIR = POOL_DIR / "D_finish_pooling"

    FINISH_DIR.mkdir(parents=True, exist_ok=True)

    ARCHIV_DIR = BASE_DIR / "archived_files"

    ARCHIV_DIR.mkdir(parents=True, exist_ok=True)

    return BASE_DIR, ISO_DIR, ISO_IN_DIR

#########################
#########################


#########################
#########################
def getWellDictionary():

    my_well_list = ['A1', 'B1', 'C1', 'D1', 'E1', 'F1', 'G1', 'H1', 'A2', 'B2', 'C2', 'D2', 'E2', 'F2', 'G2', 'H2', 'A3', 'B3', 'C3',
                    'D3', 'E3', 'F3', 'G3', 'H3', 'A4', 'B4', 'C4', 'D4', 'E4', 'F4', 'G4', 'H4', 'A5', 'B5', 'C5', 'D5', 'E5', 'F5', 'G5',
                    'H5', 'A6', 'B6', 'C6', 'D6', 'E6', 'F6', 'G6', 'H6', 'A7', 'B7', 'C7', 'D7', 'E7', 'F7', 'G7', 'H7', 'A8', 'B8', 'C8',
                    'D8', 'E8', 'F8', 'G8', 'H8', 'A9', 'B9', 'C9', 'D9', 'E9', 'F9', 'G9', 'H9', 'A10', 'B10', 'C10', 'D10', 'E10', 'F10', 'G10',
                    'H10', 'A11', 'B11', 'C11', 'D11', 'E11', 'F11', 'G11', 'H11', 'A12', 'B12', 'C12', 'D12', 'E12', 'F12', 'G12', 'H12']

    # make dict with key == well postion, and value == list position, e.g. {'A1':0, 'B1':1, ...}
    my_well_dict = {k: v for v, k in enumerate(my_well_list)}

    return my_well_list, my_well_dict

#########################
#########################



#########################
#########################
def checkCSVfiles(f, file_type_dict):
    
    # creat two tests sets with expected column headers of differetn file types
    sample_scane_set = set(['RACK',	'POS',	'BARCODE',	'STATUS'])
    
    sip_metadata_set = set(['isotope_label','sample_group_name'])
    
    # read the first line of the file and make a set of elements found space with tab delimiter
    try:
        with open(f, 'r') as file:
            # read just first line of file
            headers = file.readline()
            
            # Strip whitespace and split by comma to make a set of the column headers
            elements = set(headers.strip().split(','))
            
    except FileNotFoundError:
        print(f"Error: The file '{f}' was not found.")
    
    except Exception as e:
        print(f"An error occurred: {e}")
        
        
    # remove \" from strings in elements. This is an issue with the
    # sip-metadata file format    
    new_elements = {item.replace("\"", "") for item in elements}


    # set file type if headers in file match either the expect headers
    # of sample_scan or sip_metadata files
    if new_elements == sample_scane_set:
        
        file_type_dict['sample_scan'] = f
        
    elif sip_metadata_set.issubset(new_elements):
        
        file_type_dict['sip_metadata'] = f
        
    else:
        print(f"\nCannot determine format of file {f}.  Terminating script\n\n")
        sys.exit()
    
    # return dict where key is file type and value is the file name
    return file_type_dict

#########################
#########################


#########################
#########################
def checkXLSfiles(f, file_type_dict):
    
    # create a df from the .xls file
    tmp_df = pd.read_excel(f, header=0)
    
    # check if df columns match expectd column header of  metabolomics file
    if "Metabolomics Sample Id" in tmp_df.columns:
        
        file_type_dict['metabolomics'] = f
        
    # creat new df using the 3rd line as the header if old df has unnamed headers
    # this solves problem with Aliquote file where headers start on line 3    
    elif "Unnamed: 0" in tmp_df.columns:
        
        tmp_df = pd.read_excel(f, header=2)
        
        if "Library Queue" in tmp_df.columns:
            
            file_type_dict['aliquot'] = f
        
    else:
        print(f"\nCannot determine format of file {f}.  Terminating script\n\n")
        sys.exit()
    
    # return dict where key is file type and value is the file name
    return file_type_dict

#########################
#########################


#########################
#########################
def identifyFiles():
    
    # create dict with expect file types as keys, and None as values
    file_type_dict = dict.fromkeys(['sample_scan','sip_metadata','metabolomics','aliquot'])
    
    # make list of command line arguement files
    #sys.argv[0] is the script name itself, so we slice from index 1 onwards
    filenames = sys.argv[1:]
    
    # loop through file names
    for f in filenames:
        
        # get name and extension
        name, file_extension = os.path.splitext(f)
        
        if file_extension.lower() == ".csv":
            
            file_type_dict = checkCSVfiles(f, file_type_dict)    
                   
            
        elif file_extension.lower() == ".xls":
            
            file_type_dict = checkXLSfiles(f, file_type_dict)
            
        else:
            print(f"\nUnexpected file {f} entered as argument.  Terminating script\n\n")
            sys.exit()
            
    # confirm that each needed file type was found, otherwise stop script
    for key, value in file_type_dict.items():
        if value is None:
            print(f"\nMissing expect input file for {key} file type. Terminating script\n\n")
            sys.exit()
        
   
    return file_type_dict
#########################
#########################



########################
#########################
def mergeAliquotSSmini():
    # # Read in Clarity SM QC sheet, ignore first 2 rows and make 4th the header
    # # read barcodes as strings instead of numbers
    # clarity_df = pd.read_excel(file_type_dict['aliquot'], header=2, converters={
    #                            'ITS Sample ID': str, 'Source Barcode': str, 'Destination Barcode': str})
    
    # Read in Clarity SM QC sheet, ignore first 2 rows and make 4th the header
    # read barcodes as strings instead of numbers
    clarity_df = pd.read_excel(file_type_dict['aliquot'], header=2, converters={
                               'ITS Sample ID': str, 'Source Barcode': str, 'Destination Barcode': str})

    # # TESTING: direct link to aliquot file rather than user input
    # clarity_df = pd.read_excel('Aliquot_Creation_Meta_QC.xls', header=2, converters={
    #     'ITS Sample ID': str, 'Source Barcode': str, 'Destination Barcode': str})

    clarity_df = clarity_df[["ITS Sample ID", "Source Barcode",
                             "Fluorometer Concentration (ng/ul)", "Available Volume (ul)", "Available Mass (ng)", "Destination Barcode"]]

    clarity_df['Fluorometer Concentration (ng/ul)'] = clarity_df['Fluorometer Concentration (ng/ul)'].round(
        decimals=1)
    clarity_df['Available Volume (ul)'] = clarity_df['Available Volume (ul)'].round(
        decimals=1)
    clarity_df['Available Mass (ng)'] = clarity_df['Available Mass (ng)'].astype(
        int)

    # # read in samplescanmini output file.  Read barcode and tube location (aka TUBE) as strings
    # ssmini_df = pd.read_csv(file_type_dict['sample_scan'], converters={
    #                         'BARCODE': str, 'POS': str, 'RACK': str})
    
    # read in samplescanmini output file.  Read barcode and tube location (aka TUBE) as strings
    ssmini_df = pd.read_csv(file_type_dict['sample_scan'] , converters={
                            'BARCODE': str, 'POS': str, 'RACK': str})

    # # TESTING: read in samplescanmini output file directly rather in user input
    # ssmini_df = pd.read_csv('TS01359409-3.csv', converters={'BARCODE': str, 'POS': str, 'RACK': str})

    # select specific columns
    ssmini_df = ssmini_df[['BARCODE', 'POS', 'RACK']]

    # remove rows without empty text in 'BARCODE' field
    ssmini_df['BARCODE'] = ssmini_df['BARCODE'].replace('', np.nan)
    ssmini_df.dropna(subset=['BARCODE'], inplace=True)

    # remove whitespace from samplescan df
    ssmini_df['BARCODE'] = ssmini_df['BARCODE'].str.strip()
    ssmini_df['POS'] = ssmini_df['POS'].str.strip()
    ssmini_df['RACK'] = ssmini_df['RACK'].str.strip()

    new_pos_list = []

    # loop to reformat well positions, e.g. A01 --> A1, B01 --> B1
    for index, row in ssmini_df.iterrows():
        # print(row['POS'])
        well = row['POS']
        if well[1] == '0':
            well = well.replace('0', '')

        new_pos_list.append(well)

    # replace old well format with new format
    ssmini_df['POS'] = new_pos_list

    # merge dataframes based on 2D matrix barcode
    merged_df = clarity_df.merge(ssmini_df, how='outer', left_on='Source Barcode',
                                 right_on='BARCODE')

    # print(merged_df)
    # abort if a mismatch in barcodes is detected
    if (merged_df["BARCODE"].isnull().values.any()):
        # print("\n\n")
        # print(merged_df)
        print('\n\nAt least 1 barcode was not found. Aborting. \n\n')
        sys.exit()

    # add column using well_dict dictionary where key is TUBE postion and value is list postion
    # This will be used to sort merged_df column-wise, and reset the indexes
    merged_df['order'] = merged_df['POS'].map(well_dict)

    merged_df.sort_values(by=['order'], inplace=True)

    merged_df = merged_df.reset_index(drop=True)

    return clarity_df, merged_df
#########################
#########################


#########################
#########################
def getUserInputMinMaxParameters():
    # # ask User to input minimum volume liquid handler can transfer
    my_min_trans_vol = float(
        input("Enter the minimum volume liquid handler can pipet (default 2.4ul): ") or 2.4)

    if (my_min_trans_vol < 0):
        print('\n\nError.  Minimum volume must be >0 uL.  Aborting.\n\n')
        sys.exit()

    # user provides amount of DNA to transfer for isotope QC
    my_trans_mass = float(
        input("Enter the target transfer mass in ng's for isotope screening (default 60): ") or 60)

    # check that isotope transfer mass doesn't exceed 10% of total available DNA mass
    if (my_trans_mass >= (0.1*clarity_df['Available Mass (ng)'].min())):
        # print(
        #     clarity_df[clarity_df['Available Mass (ng)'] <= (my_trans_mass*10)])
        print("\nIsotope transfer DNA mass is >=10% of total available DNA for at least one sample. \nDo you wish to continue (Y/N)? :")

        val = input()

        if (val == 'Y' or val == 'y'):
            print("\nOk, we'll keep going")

        elif (val == 'N' or val == 'n'):
            print('\nOk, aborting script.  Try reducing transfer mass value\n\n')
            sys.exit()
        else:
            print("Sorry, you must choose 'Y' or 'N' next time. \n\nAborting\n\n")
            sys.exit()

    # user provide minimum volume for each sample.  Buffer will be added to sample
    # to bring volume up to minimum value
    my_min_sample_vol = float(
        input("\nEnter the minimum volume for sample matrix tubes (default 60ul): ") or 60)

    if (my_min_sample_vol < 0):
        print('\n\nError.  Minimum volume must be >0 uL.  Aborting.\n\n')
        sys.exit()

    my_max_dna_conc = float(
        input("\nEnter the max target DNA conc for sample matrix tubes (default 25ng/ul): ") or 25)

    if (my_max_dna_conc < 0):
        print('\n\nError.  Maximum conc must be >0 ng/uL.  Aborting.\n\n')
        sys.exit()

    return my_min_trans_vol, my_trans_mass, my_min_sample_vol, my_max_dna_conc

#########################
#########################


#########################
#########################
def calcDnaBufferTransfers(merged_df):
    # determine volume of buffer needed to top up sample matrix tubes before starting isotope plate transfers
    merged_df['Top_up_vol_(ul)'] = np.where(merged_df['Fluorometer Concentration (ng/ul)'] > max_dna_conc,
                                            (merged_df['Fluorometer Concentration (ng/ul)'] *
                                             merged_df['Available Volume (ul)'] / max_dna_conc)
                                            - merged_df['Available Volume (ul)'], 0)

    # if sample conc is below target conc and available vol is below minimum targe sample volume, then bring up volume to minimum target
    merged_df['Top_up_vol_(ul)'] = np.where(((merged_df['Available Volume (ul)'] + merged_df['Top_up_vol_(ul)']) < min_sample_vol),
                                            (min_sample_vol -
                                             merged_df['Available Volume (ul)']),
                                            merged_df['Top_up_vol_(ul)'])

    # if top up volume is too much that it will overflow tube, then adjust volume down
    merged_df['Top_up_vol_(ul)'] = np.where((merged_df['Available Volume (ul)'] + merged_df['Top_up_vol_(ul)'] > 450),
                                            (450-merged_df['Available Volume (ul)']), merged_df['Top_up_vol_(ul)'])

    # if top up volume is more than tip max volume, then adjust volume down
    merged_df['Top_up_vol_(ul)'] = np.where((merged_df['Top_up_vol_(ul)'] > 300),
                                            300, merged_df['Top_up_vol_(ul)'])

    merged_df['Top_up_vol_(ul)'] = (
        merged_df['Top_up_vol_(ul)']).round(decimals=1)

    # calcualte new available sample volume after top up
    merged_df['Updated_vol_(ul)'] = (merged_df['Available Volume (ul)'] +
                                     merged_df['Top_up_vol_(ul)'])

    # calculat new DNA conc after top up
    merged_df['Updated_conc_(ng/ul)'] = (merged_df['Fluorometer Concentration (ng/ul)'] *
                                         merged_df['Available Volume (ul)'] / merged_df['Updated_vol_(ul)']).round(decimals=1)

    # # Find sample with lowest DNA conc.  Used to determine transfer volume to isotope plate
    # lowconc = merged_df['Updated_conc_(ng/ul)'].min()

    # # determine volume to transfer to isotope plate.
    merged_df['Isotope_vol_(ul)'] = np.where((trans_mass/merged_df['Updated_conc_(ng/ul)']) >
                                             min_trans_vol, (trans_mass/merged_df['Updated_conc_(ng/ul)']), min_trans_vol)

    merged_df = merged_df.round({'Isotope_vol_(ul)': 1})

    # add column showing amount of DNA added to isotope plates
    merged_df['Isotope_mass_(ng)'] = merged_df['Isotope_vol_(ul)'] * \
        merged_df['Updated_conc_(ng/ul)']

    merged_df = merged_df.round({'Isotope_mass_(ng)': 0})

    # # calculate remaining volume
    merged_df['Remain_vol_(ul)'] = (merged_df['Updated_vol_(ul)'] -
                                    merged_df["Isotope_vol_(ul)"]).round(1)

    merged_df['Remain_mass_(ng)'] = (merged_df['Remain_vol_(ul)'] *
                                     merged_df['Updated_conc_(ng/ul)']).astype(int)

    return merged_df

#########################
#########################


#########################
#########################
def setupFAplate(merged_df, well_list, control_df):
    # calculte amount of buffer to add to isotope destination plate
    FA_trans_vol = float(
        input("Enter the volume to transfer for FA plate (default 2.4ul): ") or 2.4)

    merged_df['Buffer_isotope_vol_(ul)'] = 45 + \
        FA_trans_vol - merged_df['Isotope_vol_(ul)']

    merged_df = merged_df.round({'Buffer_isotope_vol_(ul)': 1})

    # set minimal volume for Buffer
    merged_df['Buffer_isotope_vol_(ul)'] = np.where(
        merged_df['Buffer_isotope_vol_(ul)'] < 8, 8, merged_df['Buffer_isotope_vol_(ul)'])

    # make sure buffer + dna doesn't overflow plate well... 200ul
    merged_df['Buffer_isotope_vol_(ul)'] = np.where(
        (merged_df['Buffer_isotope_vol_(ul)'] + merged_df['Isotope_vol_(ul)'] + FA_trans_vol) > 200, (200-merged_df['Isotope_vol_(ul)'] - FA_trans_vol), merged_df['Buffer_isotope_vol_(ul)'])

    # add column with barcode of Dilution plates used to later FA plates
    merged_df['Dilute_plate_barcode'] = merged_df['Destination Barcode'].astype(
        str)+'.D'

    # add column with barcode of Fragement analyzer plate
    merged_df['FA_plate_barcode'] = merged_df['Destination Barcode'].astype(
        str)+'.F'

    # add column with volume to transfer from isotope plate to Dilution plate
    merged_df['Dilute_DNA_vol_(ul)'] = FA_trans_vol

    # add column with volume of buffer to pre-load into Dilution plate. Bring volume up to 10ul
    merged_df['Dilute_buffer_vol_(ul)'] = 10 - merged_df['Dilute_DNA_vol_(ul)']

    # # add column with well location of DNA transfered to isotope plate
    # merged_df['Destination Location'] = pd.DataFrame(
    #     well_list[0:(merged_df.shape[0])])

    # # add column with well location of DNA transfered to isotope plate
    # # samples start in 7th well (G1) to leave  space for ecoli isotope standards
    # merged_df['Destination Location'] = pd.DataFrame(
    #     well_list[6:(merged_df.shape[0]+6)])

    # add column with well location of DNA transfered to isotope plate
    # samples start in 7th well (G1) to leave  space for ecoli isotope standards

    control_num = control_df.shape[0]

    merged_df['Destination Location'] = pd.DataFrame(
        well_list[control_num:(merged_df.shape[0]+control_num)])

    merged_df = merged_df.rename(columns={'POS': "Tube_location", "RACK": "Rack_Barcode",
                                          "Destination Barcode": "Isotope_plate_barcode",
                                          "Destination Location": "Isotope_well",
                                          "Fluorometer Concentration (ng/ul)": "Original_conc_(ng/ul)",
                                          "ITS Sample ID": "ITS_sample_id",
                                          "Source Barcode": "Matrix_barcode",
                                          "Available Volume (ul)": "Original_vol_(ul)",
                                          "Available Mass (ng)": "Available_mass_(ng)"})

    return merged_df
#########################
#########################


#########################
#########################
def getIsotopeProjectDfs():
    isotope_df = merged_df[['ITS_sample_id', 'Matrix_barcode', 'Rack_Barcode', 'Tube_location', 'Original_conc_(ng/ul)', 'Original_vol_(ul)', 'Top_up_vol_(ul)', 'Updated_conc_(ng/ul)', 'Updated_vol_(ul)', 'Available_mass_(ng)', 'Isotope_plate_barcode',
                            'Isotope_well', 'Isotope_mass_(ng)', 'Isotope_vol_(ul)',  'Buffer_isotope_vol_(ul)', 'Dilute_plate_barcode', 'Dilute_DNA_vol_(ul)', 'Dilute_buffer_vol_(ul)', 'FA_plate_barcode']].copy()

    
    isotope_df['FA_plate_well'] = isotope_df['Isotope_well']
    
    isotope_df['Dilution_plate_well'] = isotope_df['Isotope_well']
    
    isotope_df = isotope_df[['ITS_sample_id', 'Matrix_barcode', 'Rack_Barcode', 'Tube_location', 'Original_conc_(ng/ul)', 'Original_vol_(ul)', 'Top_up_vol_(ul)', 'Updated_conc_(ng/ul)', 'Updated_vol_(ul)', 'Available_mass_(ng)', 'Isotope_plate_barcode',
                            'Isotope_well', 'Isotope_mass_(ng)', 'Isotope_vol_(ul)',  'Buffer_isotope_vol_(ul)', 'Dilute_plate_barcode', 'Dilution_plate_well','Dilute_DNA_vol_(ul)', 'Dilute_buffer_vol_(ul)', 'FA_plate_barcode','FA_plate_well']]

    
    
    project_df = merged_df[['ITS_sample_id', 'Matrix_barcode', 'Rack_Barcode', 'Tube_location', 'Remain_mass_(ng)',
                            'Remain_vol_(ul)', 'Updated_conc_(ng/ul)', 'Isotope_plate_barcode', 'Isotope_well']].copy()

    project_df = project_df.rename(columns={"Remain_vol_(ul)": "Available_vol_(ul)",
                                            "Remain_mass_(ng)": "Available_mass_(ng)"})

    # add columns to record number to attempts a loading centrifuge, successfully merging density/volume/dna_conc files, and if libs were made
    project_df['Ultracentrifuge_attempts_(#)'] = 0
    project_df['Merged_files'] = 0
    project_df['Made_Library'] = 0

    # move aliquot and sample scan mini files from main project directory to isotope subdirectory
    # Extract just the filename from the path (handles both GUI full paths and command line filenames)
    sample_scan_filename = Path(file_type_dict['sample_scan']).name
    aliquot_filename = Path(file_type_dict['aliquot']).name
    
    sample_scan_source = Path(sample_scan_filename)
    aliquot_source = Path(aliquot_filename)
    
    sample_scan_dest = ISO_INPUT_DIR / sample_scan_filename
    aliquot_dest = ISO_INPUT_DIR / aliquot_filename
    
    try:
        if sample_scan_source.exists():
            sample_scan_source.rename(sample_scan_dest)
            print(f"✓ Moved {sample_scan_filename} to input_files directory")
        else:
            print(f"⚠ Warning: Could not find {sample_scan_filename}")
    except Exception as e:
        print(f"✗ Error moving {sample_scan_filename}: {e}")
    
    try:
        if aliquot_source.exists():
            aliquot_source.rename(aliquot_dest)
            print(f"✓ Moved {aliquot_filename} to input_files directory")
        else:
            print(f"⚠ Warning: Could not find {aliquot_filename}")
    except Exception as e:
        print(f"✗ Error moving {aliquot_filename}: {e}")

    return isotope_df, project_df

#########################
#########################


#########################
#########################
def makeIsotopeControlDf():

    # create dataframe with info on JGI internal ecoli standards used for metabolomics QC
    control_list = [['', '', 'A1', 'Ecoli_13C', 'JGI_internal_control', 'C13', 'delete whole row before uploading to Clarity', '', '', '', 60], ['', '', 'B1', 'Ecoli_13C', 'JGI_internal_control', 'C13', 'delete whole row before uploading to Clarity', '', '', '', 60], ['', '', 'C1', 'Ecoli_13C', 'JGI_internal_control', 'C13', 'delete whole row before uploading to Clarity', '', '', '', 60], [
        '', '', 'D1', 'Ecoli_12C', 'JGI_internal_control', 'Unlabeled', 'delete whole row before uploading to Clarity', '', '', '', 60], ['', '', 'E1', 'Ecoli_12C', 'JGI_internal_control', 'Unlabeled', 'delete whole row before uploading to Clarity', '', '', '', 60], ['', '', 'F1', 'Ecoli_12C', 'JGI_internal_control', 'Unlabeled', 'delete whole row before uploading to Clarity', '', '', '', 60]]

    # # create dataframe with info on JGI internal ecoli standards used for metabolomics QC
    # control_list = [['', '', 'A1', 'Ecoli_13C', 'JGI_internal_control', 'C13', 'delete whole row before uploading to Clarity', '', '', '', 60], ['', '', 'B1', 'Ecoli_13C', 'JGI_internal_control', 'C13', 'delete whole row before uploading to Clarity', '', '', '', 60], ['', '', 'C1', 'Ecoli_13C', 'JGI_internal_control', 'C13', 'delete whole row before uploading to Clarity', '', '', '', 60], [
    #     '', '', 'D1', 'Ecoli_12C', 'JGI_internal_control', 'Unlabeled', 'delete whole row before uploading to Clarity', '', '', '', 60]]

    # control_list = []
    control_df = pd.DataFrame(control_list, columns=['PMOS Sample ID',	'Plate Barcode', 'Well', 'Name passed for fraction FD name creation',
                              'Group',	'Label', 'Isotope Enrichment (at%)',	'Metabolomics Sample Id', 'Project_ID', 'PI_name', 'DNA_mass_(ng)'])
    return control_df
#########################
#########################


#####################
#####################
def updateLibCreationFile(my_sheet, my_merged_df):

    my_sheet.write(0, 8, 'Project_ID')

    my_sheet.write(0, 9, 'PI_name')

    my_sheet.write(0, 10, 'DNA_mass_(ng)')

    # overwrite all cell values from row 0-500 and col 1-21 to create blank slate in lib info area of .xls file
    for r in range(1, 500, 1):
        for c in range(0, 50, 1):
            my_sheet.write(r, c, '')

    # loop through merged df and write info into .xls file
    for row_num, row in my_merged_df.iterrows():
        for col_num, col in enumerate(row):
            my_sheet.write(1+row_num, col_num, col)

    return my_sheet

#####################
#####################


############################
############################
def updateProjectDatabase(p_df, parent_all_inclusive):
    # read in all columns of all_inclusive file into df
    all_df = pd.read_csv(parent_all_inclusive, converters={
                           'source_sample_id': str, 'proposal_id':str})

    # find and remove rows with 'abandoned' in any column
    mask = np.column_stack([all_df[col].astype(str).str.contains(
        '.*Abandoned.*', na=False, case=False) for col in all_df])

    # note the ~ to select rows that are not in mask
    all_df = all_df.loc[~mask.any(axis=1)]

    # split PI name based on first comma
    all_df[['PI_name', 'First_name']] = all_df['proposal_pi'].str.split(
        ',', expand=True, n=1)

    # rearrange columns and keep only desired columns
    all_df = all_df[['proposal_id', 'PI_name',
                     'source_final_deliv_project_name', 'source_sample_id', 'sample_group_name', 'isotope_label']]
    
    
    all_df = all_df.sort_values(by=['source_sample_id'])
    
    all_df = all_df.reset_index(drop=True)

    # rename columns that will ultimately be kept by removing white spaces
    all_df.rename(columns={'proposal_id': 'Proposal_ID', 'source_final_deliv_project_name': 'Sample_Name', 'source_sample_id':'Sample_ID',
                  'sample_group_name': 'Replicate_Group'}, inplace=True)

    # check of project_database.csv already has
    if ('Sample_Name' in p_df.columns) or ('Replicate_Group' in p_df.columns):
        print('\n\nThe project_database.csv file already had columns with Sample Name and/or Replicate Group.  Do you want to overwrite?')

        val = input()

        if (val == 'Y' or val == 'y'):
            print("Ok, we'll keep going\n\n")

            # remove existing names and replicate groups
            p_df = p_df.drop(columns=['Sample_Name', 'Replicate_Group'])

        elif (val == 'N' or val == 'n'):
            print('Ok, aborting script\n\n')
            sys.exit()
        else:
            print("Sorry, you must choose 'Y' or 'N' next time. \n\nAborting\n\n")
            sys.exit()

    # # merege all inclused df with project database df based on sample id and tube barcodes
    # my_updated_project_df = all_df.merge(p_df, how='outer', left_on=[
    #     'Sample ID', 'Barcode'], right_on=['ITS_sample_id', 'Matrix_barcode'])

    # merege all inclused df with project database df based on sample id and tube barcodes
    my_updated_project_df = all_df.merge(p_df, how='outer', left_on=[
        'Sample_ID'], right_on=['ITS_sample_id'])

    # confirm that all sample is and tube barcodes in all_inclusive.xlsx were found in project_database.csv and vice versa
    if my_updated_project_df.shape[0] != all_df.shape[0]:
        print('\n\nThere was a mismatch in sample id or tube barcode when merging all_inclusive.xlsx info with project_datavase.csv.  Aborting process:')
        sys.exit()

    # drop redundant columns used during merge
    my_updated_project_df = my_updated_project_df.drop(columns=['Sample_ID'])

    # move sip_metadata file to input_files directory
    # Extract just the filename from the path (handles both GUI full paths and command line filenames)
    sip_metadata_filename = Path(parent_all_inclusive).name
    sip_metadata_source = Path(sip_metadata_filename)
    sip_metadata_dest = ISO_INPUT_DIR / sip_metadata_filename
    
    try:
        if sip_metadata_source.exists():
            sip_metadata_source.rename(sip_metadata_dest)
            print(f"✓ Moved {sip_metadata_filename} to input_files directory")
        else:
            print(f"⚠ Warning: Could not find {sip_metadata_filename}")
    except Exception as e:
        print(f"✗ Error moving {sip_metadata_filename}: {e}")

    return my_updated_project_df
############################
############################

#########################
#########################


def updateMetabolomics(merged_df, my_p_df, control_df):

    metab_file = file_type_dict['metabolomics']

    metabolomics_df = pd.read_excel(
        metab_file, header=0, converters={'PMOS Sample ID': str})

    iso_well_df = merged_df[['ITS_sample_id',
                             'Isotope_well', 'Isotope_mass_(ng)']]

    # merge dataframes based on 2D matrix barcode
    updated_df = metabolomics_df.merge(
        iso_well_df, how='outer', left_on='PMOS Sample ID', right_on='ITS_sample_id')

    # error if not all PMOS and ITS matched up
    if (updated_df["PMOS Sample ID"].isnull().values.any()):
        # print("\n", updated_df, "\n\n")
        print("""There was a mismatch between metabolomics PMOS IDs and aliquot ITS IDs\n\nSee table above.  Aborting script""")
        sys.exit()

    updated_df['Well'] = updated_df['Isotope_well']

    # add column using well_dict dictionary where key is Well postion and value is list postion
    # This will be used to sort updated_df column-wise by well position, and reset the indexes
    updated_df['order'] = updated_df['Well'].map(well_dict)

    updated_df.sort_values(by=['order'], inplace=True)

    updated_df = updated_df.reset_index(drop=True)

    # rename columns so matches expected format for metabolomics isotope QC
    # repurposing df columns for ITS sample id and Isotope well to hole the Project ID and PI name
    updated_df = updated_df.rename(
        columns={"ITS_sample_id": "Project_ID", "Isotope_well": "PI_name", "Isotope_mass_(ng)": "DNA_mass_(ng)"})

    # # have operator provide the project ID and PI name
    # proj_id = input("\n Please type in project ID:\n\n")

    # pi_name = input("\n Enter the PI name: ")

    # get proposal ID and PI name from updated_project_df
    proj_id = my_p_df.iloc[0]['Proposal_ID']
    pi_name = my_p_df.iloc[0]['PI_name']

    updated_df['Project_ID'] = proj_id
    updated_df['PI_name'] = pi_name

    # # removed unneeded columns
    updated_df = updated_df.drop(columns=['order'])

    updated_df['PMOS Sample ID'] = updated_df['PMOS Sample ID'].astype(int)

    updated_df['Project_ID'] = updated_df['Project_ID'].astype(int)

    updated_df['Isotope Enrichment (at%)'] = ''

    updated_df['Metabolomics Sample Id'] = ''

    # concat with ecoli control df
    updated_control_df = pd.concat([control_df, updated_df], ignore_index=True)

    # read in library creation file
    book = xlrd.open_workbook(metab_file)

    # make copy of workbook with xlwt module so that it can be editted
    wb = copy(book)

    # select current worksheet in wlwt from wb copy of workbook
    s = wb.get_sheet("Sheet1")

    # update worksheet with library summary stats and metadata
    s = updateLibCreationFile(s, updated_control_df)

    # make new lib_creation.xls filled out with library metadata
    wb.save(ISO_OUTPUT_DIR /
            f'updated_Metabolomics_QC_{proj_id}_{pi_name}.xls')

    # move original metabolmics file to "input" subfolder
    # Extract just the filename from the path (handles both GUI full paths and command line filenames)
    metabolomics_filename = Path(metab_file).name
    metabolomics_source = Path(metabolomics_filename)
    metabolomics_dest = ISO_INPUT_DIR / metabolomics_filename
    
    try:
        if metabolomics_source.exists():
            metabolomics_source.rename(metabolomics_dest)
            print(f"✓ Moved {metabolomics_filename} to input_files directory")
        else:
            print(f"⚠ Warning: Could not find {metabolomics_filename}")
    except Exception as e:
        print(f"✗ Error moving {metabolomics_filename}: {e}")
#########################
#########################


#########################
#########################
def printBarcodes(project_df):
    # creates lists of source and destination plate barcodes
    dest_list = project_df['Isotope_plate_barcode'].unique().tolist()

    # this was older format for bartender templates.  The newer version below changes "/" to "\"
    # in the path to the template files  AF="*"
    
    # x = '%BTW% /AF="//BARTENDER/shared/templates/ECHO_BCode8.btw" /D="%Trigger File Name%" /PRN="bcode8" /R=3 /P /DD\r\n\r\n%END%\r\n\r\n\r\n'

    # add info to start of barcode print file indicating the template and printer to use
    x = '%BTW% /AF="\\\BARTENDER\shared\\templates\ECHO_BCode8.btw" /D="%Trigger File Name%" /PRN="bcode8" /R=3 /P /DD\r\n\r\n%END%\r\n\r\n\r\n'


    bc_file = open(ISO_OUTPUT_DIR / "isotope_FA_barcodes.txt", "w")
    bc_file.writelines(x)

    # add barcodes of library destination plates, dna source plates, and buffer plate
    for p in dest_list:
        bc_file.writelines(f'{p},{p}\r\n{p}.D,{p}.D\r\n{p}.F,{p}.F\r\n')

    bc_file.close()
#########################
#########################


#########################
#########################
def makeFAUploadFiles(merged_df, well_list_96w):

    # get list of all plates for FA upload
    dest_list = merged_df['Isotope_plate_barcode'].unique().tolist()

    # get two columns from merged_df
    FA_df = merged_df[['Isotope_well', 'ITS_sample_id']].copy()

    # make temporary df of just 96 wells
    tmp_df = pd.DataFrame(well_list_96w)

    tmp_df.columns = ["Well"]

    # merge temp df of 96wells with df of project samples
    tmp_df = tmp_df.merge(FA_df, how='outer', left_on=['Well'],
                          right_on=['Isotope_well'])

    tmp_df = tmp_df.drop(columns=['Isotope_well'])

    # reset index so begins at 1 insted of 0
    tmp_df.index = range(1, tmp_df.shape[0]+1)

    # # T.reindex(T.index.union(missing))

    # missing = [*range(0, 96, 1)]

    # FA_df = FA_df.reindex(FA_df.index.union(missing))

    # # update index to start at 1 instead of 0
    # FA_df.index = range(1, FA_df.shape[0] + 1)

    # identify ecoli internal standards
    tmp_df.loc[0:control_df.shape[0], 'ITS_sample_id'] = 'Ecoli_control'

    tmp_df['ITS_sample_id'] = tmp_df['ITS_sample_id'].fillna('empty_well')

    tmp_df.loc[tmp_df.Well == 'H12', 'ITS_sample_id'] = "ladder_1"

    # create .csv file for uploading to Fragement Analyzer
    tmp_df.to_csv(ISO_OUTPUT_DIR /
                  f'FA_input_{dest_list[0]}.csv', index=True, header=False)


#########################
#########################




#########################
#########################
def createSQLdb(result_df):
    

    sql_db_path = BASE_OUTPUT_DIR /'project_database.db'

    engine = create_engine(f'sqlite:///{sql_db_path}') 


    # Specify the table name and database engine
    table_name = 'project_database'
    
    # Export the DataFrame to the SQLite database
    result_df.to_sql(table_name, engine, if_exists='replace', index=False) 

    engine.dispose()

    return
#########################
#########################



#########################
# MAIN PROGRAM
#########################
# check if all required input files were provided and exist
if len(sys.argv) < 5:
    print('\n\nDid not provide all required input files. Aborting. \n\n')
    sys.exit()
else:

    # loop through all provided input files and confirm they exist
    for s in sys.argv:
        if (file_exists(s) == 0):
            print(f'\n\nCould not find file {s} \nAborting\n\n')
            sys.exit()



# make dict were key is file type (e.g. 'sample_scan' or 'aliquot', and values are file names)
file_type_dict = identifyFiles()


# # assigne parent_all_inclusive file
# parent_all_inclusive = file_type_dict['sip_metadata']

# # assigne parent_all_inclusive file
parent_all_inclusive = file_type_dict['sip_metadata']


# create all sub folders for project folder
#BASE_DIR, ISO_DIR, ISO_IN_DIR
BASE_OUTPUT_DIR, ISO_OUTPUT_DIR, ISO_INPUT_DIR = createDirectories()

# get a list of wells and a dict where key is well location in 96 format and value is location in 384 format
well_list, well_dict = getWellDictionary()

# create dataframe for JGI internal ecoli controls to be used in
# metabolomics QC
control_df = makeIsotopeControlDf()

# clarity_df, merged_df = mergeAliquotSSmini(file_type_dict['aliquot'], file_type_dict['sample_scan'], well_dict)
clarity_df, merged_df = mergeAliquotSSmini()

# get info about transfer mass, target DNA conc for each sample after top up, and minimal transfer volumes
# based on liquid handler specs
min_trans_vol, trans_mass, min_sample_vol, max_dna_conc = getUserInputMinMaxParameters()

# add columns to df about transfer volumes etc for isotope plate
merged_df = calcDnaBufferTransfers(merged_df)

# add columsn to df about transfer volumec for FA dilution plate
merged_df = setupFAplate(merged_df, well_list, control_df)

# get df's to be used for isotope transfer file and creating overall project database
isotope_df, project_df = getIsotopeProjectDfs()


# add Project ID, PI name, Sample Name, and Sample Replicate Group to project_database.csv
updated_project_df = updateProjectDatabase(
    project_df, parent_all_inclusive)


# update the .xls file provided to metabolomics group
# updating well postion and adding project ID and PI name to form
updateMetabolomics(merged_df, updated_project_df, control_df)

# print barcodes for isotope and dilute/fa plates
printBarcodes(project_df)

# generate files for uploading to FA
makeFAUploadFiles(merged_df, well_list_96w)

##### print output files  ####
isotope_df.to_csv(ISO_OUTPUT_DIR / 'isotope_transfer.csv', index=False)

# project_df.to_csv('project_database.csv', index=False, line_terminator='\r\n')
updated_project_df.to_csv('project_database.csv', index=False)

# make sqlite db for the project_database
createSQLdb(updated_project_df)

# SUCCESS MARKER: Create success marker file to indicate script completed successfully
import datetime
script_name = Path(__file__).stem  # Use just the filename without extension
status_dir = Path(".workflow_status")
status_dir.mkdir(exist_ok=True)

# Remove any existing success file (for re-runs)
success_file = status_dir / f"{script_name}.success"
if success_file.exists():
    success_file.unlink()

# Create success marker file
try:
    with open(success_file, "w") as f:
        f.write(f"SUCCESS: {script_name} completed at {datetime.datetime.now()}")
    print(f"✓ Step {script_name} completed successfully")
except Exception as e:
    print(f"✗ Failed to create success marker: {e}")
    sys.exit(1)
